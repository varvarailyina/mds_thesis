{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS Thesis\n",
    "#### 01. Pre-process the PARTYPRESS data\n",
    "\n",
    "<br>\n",
    "<hr style=\"opacity: 0.5\">\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/varvarailyina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from datasets import Dataset\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check wd\n",
    "os.getcwd()\n",
    "#os.chdir(\"/Users/varvarailyina/hertie/mds_thesis/scripts/\")\n",
    "\n",
    "# load labels data\n",
    "df_partypress = pd.read_csv(\"../data/in/partypress/csv/partypress.csv\")\n",
    "\n",
    "# load text data\n",
    "df_texts = pd.read_csv(\"../data/in/partypress/csv/partypress_texts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"opacity: 0.5\">\n",
    "\n",
    "### Pre-process data\n",
    "\n",
    "-- *Merge PARTYPRESS datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check indices\n",
    "print(df_partypress.index.is_unique)\n",
    "print(df_texts.index.is_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set merging index to be 'id'\n",
    "df_partypress.set_index('id', inplace=True)\n",
    "df_texts.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets based on id and country_name\n",
    "df = df_partypress.merge(df_texts, on=['id', 'country_name'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['country_name', 'parlgov_id', 'party', 'party_name',\n",
      "       'party_name_english', 'family_name', 'date', 'month', 'month_start',\n",
      "       'month_end', 'calendar_week', 'week_start', 'week_end', 'header',\n",
      "       'issue_multi', 'issue_mono', 'issue', 'issue_coder2', 'position',\n",
      "       'position_coder2', 'cv_sample', 'issue_ridge', 'issue_super', 'text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# check var names\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- *Filter data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for germany\n",
    "df_de = df[df['country_name'] == 'germany']\n",
    "\n",
    "# filter out press releases with no issue (use Monolingual Transformer)\n",
    "df_de = df_de[df_de['issue_mono'].isin([98, 99]) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 43685 entries, 18021 to 10366\n",
      "Data columns (total 24 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   country_name        43685 non-null  object \n",
      " 1   parlgov_id          43685 non-null  float64\n",
      " 2   party               43685 non-null  object \n",
      " 3   party_name          43685 non-null  object \n",
      " 4   party_name_english  43685 non-null  object \n",
      " 5   family_name         43685 non-null  object \n",
      " 6   date                43685 non-null  object \n",
      " 7   month               43685 non-null  int64  \n",
      " 8   month_start         43685 non-null  object \n",
      " 9   month_end           43685 non-null  object \n",
      " 10  calendar_week       43685 non-null  int64  \n",
      " 11  week_start          43685 non-null  object \n",
      " 12  week_end            43685 non-null  object \n",
      " 13  header              43685 non-null  object \n",
      " 14  issue_multi         43685 non-null  int64  \n",
      " 15  issue_mono          43685 non-null  int64  \n",
      " 16  issue               2528 non-null   float64\n",
      " 17  issue_coder2        332 non-null    float64\n",
      " 18  position            2519 non-null   float64\n",
      " 19  position_coder2     315 non-null    float64\n",
      " 20  cv_sample           43685 non-null  int64  \n",
      " 21  issue_ridge         43685 non-null  int64  \n",
      " 22  issue_super         43685 non-null  int64  \n",
      " 23  text                43685 non-null  object \n",
      "dtypes: float64(5), int64(7), object(12)\n",
      "memory usage: 8.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_de.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- *Add issue lables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify issue labels\n",
    "issue_mapping = {\n",
    "    1: \"Macroeconomics\",\n",
    "    2: \"Civil Rights\",\n",
    "    3: \"Health\",\n",
    "    4: \"Agriculture\",\n",
    "    5: \"Labor\",\n",
    "    6: \"Education\",\n",
    "    7: \"Environment\",\n",
    "    8: \"Energy\",\n",
    "    9: \"Immigration\",\n",
    "    10: \"Transportation\",\n",
    "    12: \"Law and Crime\",\n",
    "    13: \"Social Welfare\",\n",
    "    14: \"Housing\",\n",
    "    15: \"Domestic Commerce\",\n",
    "    16: \"Defense\",\n",
    "    17: \"Technology\",\n",
    "    18: \"Foreign Trade\",\n",
    "    20: \"Government Operations\",\n",
    "    23: \"Culture\",\n",
    "    191: \"International Affairs\",\n",
    "    192: \"European Integration\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add `issue_label` column\n",
    "df_de['issue_label'] = df_de['issue_mono'].map(issue_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define needed variables\n",
    "needed_vars = [\n",
    "    'country_name', 'party', 'party_name', 'family_name', 'date', 'month', 'calendar_week', 'issue_mono', 'issue_label', 'header', 'text'\n",
    "]\n",
    "\n",
    "# clean `df_de`\n",
    "df_de = df_de[needed_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- *Remove outliers (based on word count in press releases)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a word count column\n",
    "df_de['n_words'] = df_de['text'].str.split().apply(len)\n",
    "\n",
    "# define lower and upper cutoffs\n",
    "lower_bound = df_de['n_words'].quantile(0.025)\n",
    "upper_bound = df_de['n_words'].quantile(0.975)\n",
    "\n",
    "# remove outliers (filter within range)\n",
    "df_de = df_de[(df_de['n_words'] >= lower_bound) & (df_de['n_words'] <= upper_bound)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as .csv\n",
    "df_de.to_csv(\"../data/out/df_de.csv\", index=False)\n",
    "\n",
    "# load `df_de` data\n",
    "#df_de = pd.read_csv(\"../data/out/df_de.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Zu den heutigen Äußerungen von SPD-Fraktionschef Frank-Walter Steinmeier erklärt die stellvertretende Vorsitzende der FDP-Bundestagsfraktion Gisela PILTZ: Der SPD-Fraktionschef leidet augenscheinlich an partieller Politikamnesie. Die Menschen werden sich von diesem plumpen Versuch der SPD, von den eigenen Versäumnissen abzulenken, nicht täuschen lassen. Dass der Schutz von Arbeitnehmerdaten dringend einer Verbesserung bedarf, wissen wir nicht erst seit Lidl, Bahn und Co. Doch elf Jahre im bislang verantwortlichen und SPD-geführten Bundesarbeitsministerium haben nicht ausgereicht, um sich des akuten Handlungsbedarfes anzunehmen. Jetzt auf die FDP zu zeigen, ist ein Armutszeugnis.  Vorschläge für eine Verbesserung des Schutzes von Arbeitnehmerdaten suchte man in der Vergangenheit bei der SPD vergebens. Vollmundigen Ankündigungen folgte rein gar nichts. Auch der jüngste Entwurf für ein Arbeitnehmerdatenschutzgesetz durch die SPD kann allenfalls als mangelhaft bezeichnet werden. Eindrucksvoll beweist die SPD auf ein Neues, dass sie auch hier elf Jahre ihre Hausaufgaben nicht gemacht hat. An der Umsetzung dieses unpraktikablen Entwurfes werden weder Arbeitgeber noch Arbeitnehmer ein Interesse haben.  Die neue Bundesregierung wird in diesem Jahr die Versäumnisse des bis zuletzt zuständigen SPD-Arbeitsressorts angehen und ein verbessertes Arbeitnehmerdatenschutzrecht vorlegen.\n"
     ]
    }
   ],
   "source": [
    "# look at a press release\n",
    "print(df_de['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- *Split press releases into sentences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to split press releases into sentences\n",
    "def explode_sentences(df, text_col='text'):\n",
    "    \n",
    "    # apply sentence splitting\n",
    "    df['sentences'] = df[text_col].apply(sent_tokenize)\n",
    "    \n",
    "    # each sentence gets its own row\n",
    "    df_exploded = df.explode('sentences').reset_index(drop=True)\n",
    "    \n",
    "    # rename column\n",
    "    df_exploded = df_exploded.rename(columns={'sentences': 'sentence'})\n",
    "    \n",
    "    return df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function\n",
    "df_sentences = explode_sentences(df_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 467480 entries, 0 to 467479\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   country_name   467480 non-null  object\n",
      " 1   party          467480 non-null  object\n",
      " 2   party_name     467480 non-null  object\n",
      " 3   family_name    467480 non-null  object\n",
      " 4   date           467480 non-null  object\n",
      " 5   month          467480 non-null  int64 \n",
      " 6   calendar_week  467480 non-null  int64 \n",
      " 7   issue_mono     467480 non-null  int64 \n",
      " 8   issue_label    467480 non-null  object\n",
      " 9   header         467480 non-null  object\n",
      " 10  text           467480 non-null  object\n",
      " 11  n_words        467480 non-null  int64 \n",
      " 12  sentence       467480 non-null  object\n",
      "dtypes: int64(4), object(9)\n",
      "memory usage: 46.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_sentences.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as .csv\n",
    "df_sentences.to_csv(\"../data/out/df_sentences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as .pkl file\n",
    "df_sentences.to_pickle(\"../data/out/df_sentences.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- *Extract only sentence text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sentence text\n",
    "sentences = list(df_sentences['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sentence data as .pkl file\n",
    "with open('../data/out/sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
