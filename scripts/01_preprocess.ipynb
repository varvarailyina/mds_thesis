{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS Thesis\n",
    "#### 01. Pre-process the PARTYPRESS data\n",
    "\n",
    "<br>\n",
    "<hr style=\"opacity: 0.5\">\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/varvarailyina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from datasets import Dataset\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check wd\n",
    "os.getcwd()\n",
    "#os.chdir(\"/Users/varvarailyina/hertie/mds_thesis/scripts/\")\n",
    "\n",
    "# load labels data\n",
    "df_partypress = pd.read_csv(\"../data/in/partypress/csv/partypress.csv\")\n",
    "\n",
    "# load text data\n",
    "df_texts = pd.read_csv(\"../data/in/partypress/csv/partypress_texts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"opacity: 0.5\">\n",
    "\n",
    "### Pre-process data\n",
    "\n",
    "-- *Merge PARTYPRESS datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check indices\n",
    "print(df_partypress.index.is_unique)\n",
    "print(df_texts.index.is_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set merging index to be 'id'\n",
    "df_partypress.set_index('id', inplace=True)\n",
    "df_texts.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets based on id and country_name\n",
    "df = df_partypress.merge(df_texts, on=['id', 'country_name'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['country_name', 'parlgov_id', 'party', 'party_name',\n",
      "       'party_name_english', 'family_name', 'date', 'month', 'month_start',\n",
      "       'month_end', 'calendar_week', 'week_start', 'week_end', 'header',\n",
      "       'issue_multi', 'issue_mono', 'issue', 'issue_coder2', 'position',\n",
      "       'position_coder2', 'cv_sample', 'issue_ridge', 'issue_super', 'text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# check var names\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- *Filter data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for germany\n",
    "df_de = df[df['country_name'] == 'germany']\n",
    "\n",
    "# filter out press releases with no issue (use Monolingual Transformer)\n",
    "df_de = df_de[df_de['issue_mono'].isin([98, 99]) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 43685 entries, 18021 to 10366\n",
      "Data columns (total 24 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   country_name        43685 non-null  object \n",
      " 1   parlgov_id          43685 non-null  float64\n",
      " 2   party               43685 non-null  object \n",
      " 3   party_name          43685 non-null  object \n",
      " 4   party_name_english  43685 non-null  object \n",
      " 5   family_name         43685 non-null  object \n",
      " 6   date                43685 non-null  object \n",
      " 7   month               43685 non-null  int64  \n",
      " 8   month_start         43685 non-null  object \n",
      " 9   month_end           43685 non-null  object \n",
      " 10  calendar_week       43685 non-null  int64  \n",
      " 11  week_start          43685 non-null  object \n",
      " 12  week_end            43685 non-null  object \n",
      " 13  header              43685 non-null  object \n",
      " 14  issue_multi         43685 non-null  int64  \n",
      " 15  issue_mono          43685 non-null  int64  \n",
      " 16  issue               2528 non-null   float64\n",
      " 17  issue_coder2        332 non-null    float64\n",
      " 18  position            2519 non-null   float64\n",
      " 19  position_coder2     315 non-null    float64\n",
      " 20  cv_sample           43685 non-null  int64  \n",
      " 21  issue_ridge         43685 non-null  int64  \n",
      " 22  issue_super         43685 non-null  int64  \n",
      " 23  text                43685 non-null  object \n",
      "dtypes: float64(5), int64(7), object(12)\n",
      "memory usage: 8.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_de.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- *Add issue lables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify issue labels\n",
    "issue_mapping = {\n",
    "    1: \"Macroeconomics\",\n",
    "    2: \"Civil Rights\",\n",
    "    3: \"Health\",\n",
    "    4: \"Agriculture\",\n",
    "    5: \"Labor\",\n",
    "    6: \"Education\",\n",
    "    7: \"Environment\",\n",
    "    8: \"Energy\",\n",
    "    9: \"Immigration\",\n",
    "    10: \"Transportation\",\n",
    "    12: \"Law and Crime\",\n",
    "    13: \"Social Welfare\",\n",
    "    14: \"Housing\",\n",
    "    15: \"Domestic Commerce\",\n",
    "    16: \"Defense\",\n",
    "    17: \"Technology\",\n",
    "    18: \"Foreign Trade\",\n",
    "    20: \"Government Operations\",\n",
    "    23: \"Culture\",\n",
    "    191: \"International Affairs\",\n",
    "    192: \"European Integration\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add `issue_label` column\n",
    "df_de['issue_label'] = df_de['issue_mono'].map(issue_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define needed variables\n",
    "needed_vars = [\n",
    "    'country_name', 'party', 'party_name', 'family_name', 'date', 'month', 'calendar_week', 'issue_mono', 'issue_label', 'header', 'text'\n",
    "]\n",
    "\n",
    "# clean `df_de`\n",
    "df_de = df_de[needed_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- *Remove outliers (based on word count in press releases)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a word count column\n",
    "df_de['n_words'] = df_de['text'].str.split().apply(len)\n",
    "\n",
    "# define lower and upper cutoffs\n",
    "lower_bound = df_de['n_words'].quantile(0.025)\n",
    "upper_bound = df_de['n_words'].quantile(0.975)\n",
    "\n",
    "# remove outliers (filter within range)\n",
    "df_de = df_de[(df_de['n_words'] >= lower_bound) & (df_de['n_words'] <= upper_bound)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as .csv\n",
    "df_de.to_csv(\"../data/out/df_de.csv\", index=False)\n",
    "\n",
    "# load `df_de` data\n",
    "#df_de = pd.read_csv(\"../data/out/df_de.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- *Sample data (remove this later)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 1000 press releases\n",
    "#df_sample = df_de.sample(n=1000, random_state=42)\n",
    "\n",
    "# randomly select 8800 press releases\n",
    "df_sample = df_de.sample(n=8800, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anton Hofreiter, Fraktionsvorsitzender, ruft zur Teilnahme an der Demonstration „Der Agrarindustrie die Stirn bieten! Wir haben es satt!“ auf und erklärt zum Start der Internationalen Grünen Woche:  Minister Schmidt verpasst eine Chance. Die Internationale Grüne Woche wäre der richtige Anlass, um die notwendige Agrarwende anzukündigen. Doch der Minister setzt lieber auf ein Weiterso, mit dem das Bauernhofsterben befördert, die Natur weiter zerstört und die Gesundheit von Mensch und Tier vernachlässigt wird. Es ist nicht akzeptabel, dass Minister Schmidt den Glyphosat-Ausstieg ganz offensichtlich weiter aussitzen und das Tierleid bloß mit einem staatlichen Alibi-Labelchen überkleben will. Auch das Sondierungsergebnis von Union und SPD beweist: Schwarz-Rot will ein grundlegendes Umsteuern verhindern.  Dabei wäre eine echte Agrarwende dringend nötig, damit Artensterben, Hofaufgaben, Tierleid und nitratbelastetes Grundwasser endlich Vergangenheit sind. Statt eines Wischi-Waschi-Labels von Minister Schmidt braucht es eine verbindliche Haltungskennzeichnung für Fleisch. Statt Großbauern zu subventionieren, brauchen wir eine stärkere Förderung von Umwelt- und Tierschutz. Die Regeln für Tier- und Umweltschutz müssen deutlich verbessert werden. Doch dazu finden sich in den Sondierungsvereinbarungen höchstens Lippenbekenntnisse. Bei Glyphosat gibt es nur ein unverbindliches Bekenntnis, das schon wenige Tage später von den zuständigen Ministern Hendricks und Schmidt unterschiedlich interpretiert wird. Ob und wann der Glyphosat-Ausstieg wirklich kommt, ist damit völlig offen.  Das zeigt: Die Beharrungskräfte bei Lobby, Pestizidproduzenten, Union und SPD sind weiter stark. Es braucht öffentlichen Druck, damit es zu Veränderungen kommt. Deshalb ist die „Wir haben es satt“-Demo in diesem Jahr so wichtig. Die Bundestagsfraktion von Bündnis 90/Die Grünen steht auch in diesem Jahr an der Seite der Demonstrantinnen und Demonstranten und ruft zur Teilnahme an der Demo auf. Wir protestieren am Samstag gemeinsam mit tausend anderen Menschen für die Agrarwende, für eine ökologischere Landwirtschaft, für gesundes Essen und gegen die industrielle Massentierhaltung und den Einsatz von Glyphosat auf unseren Feldern.\n"
     ]
    }
   ],
   "source": [
    "# look at a press release\n",
    "print(df_sample['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- *Split press releases into sentences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to split press releases into sentences\n",
    "def explode_sentences(df, text_col='text'):\n",
    "    \n",
    "    # apply sentence splitting\n",
    "    df['sentences'] = df[text_col].apply(sent_tokenize)\n",
    "    \n",
    "    # each sentence gets its own row\n",
    "    df_exploded = df.explode('sentences').reset_index(drop=True)\n",
    "    \n",
    "    # rename column\n",
    "    df_exploded = df_exploded.rename(columns={'sentences': 'sentence'})\n",
    "    \n",
    "    return df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function\n",
    "#df_sentences = explode_sentences(df_de)\n",
    "df_sentences = explode_sentences(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as .csv\n",
    "df_sentences.to_csv(\"../data/out/df_sentences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- *Extract only sentence text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sentence text\n",
    "sentences = list(df_sentences['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sentence data as .pkl file\n",
    "with open('../data/out/sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
