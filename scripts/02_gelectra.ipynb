{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS Thesis\n",
    "#### 02. Use pre-trained GELECTRA model\n",
    "\n",
    "<br>\n",
    "<hr style=\"opacity: 0.5\">\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from transformers import ElectraTokenizer, ElectraModel\n",
    "from datasets import Dataset, load_from_disk\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/varvarailyina/hertie/mds_thesis/scripts'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check wd\n",
    "os.getcwd()\n",
    "\n",
    "#os.chdir(\"/Users/varvarailyina/hertie/mds_thesis/scripts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "#df_de = pd.read_csv(\"../data/out/df_de.csv\")\n",
    "df_sample = pd.read_csv(\"../data/out/df_sample.csv\")\n",
    "\n",
    "# load dataset\n",
    "dataset = load_from_disk(\"../data/out/dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraModel(\n",
       "  (embeddings): ElectraEmbeddings(\n",
       "    (word_embeddings): Embedding(32767, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): ElectraEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tokenizer and model\n",
    "tokenizer = ElectraTokenizer.from_pretrained('german-nlp-group/electra-base-german-uncased')\n",
    "model = ElectraModel.from_pretrained('german-nlp-group/electra-base-german-uncased')\n",
    "\n",
    "# set to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"opacity: 0.5\">\n",
    "\n",
    "### Tokenization and embeddings\n",
    "\n",
    "-- *Tokenize data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'country_name', 'parlgov_id', 'party', 'party_name',\n",
      "       'party_name_english', 'family_name', 'date', 'month', 'month_start',\n",
      "       'month_end', 'calendar_week', 'week_start', 'week_end', 'header',\n",
      "       'issue_multi', 'issue_mono', 'issue', 'issue_coder2', 'position',\n",
      "       'position_coder2', 'cv_sample', 'issue_ridge', 'issue_super', 'text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# keep index as id\n",
    "df_sample.reset_index(inplace=True)\n",
    "df_sample.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "print(df_sample.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def tokenize_function(text):\n",
    "#    return tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "# apply tokenization and keep 'id'\n",
    "#df_sample['tokenized_data'] = df_sample['text'].apply(tokenize_function)\n",
    "#df_sample['id'] = df_sample.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8800/8800 [00:47<00:00, 184.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# define function to tokenize data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512, return_tensors='pt')  # return_tensors='pt' for PyTorch\n",
    "\n",
    "# run function\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- *Extract embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8800/8800 [2:16:10<00:00,  1.08 examples/s]  \n"
     ]
    }
   ],
   "source": [
    "# define function to extract embeddings\n",
    "def extract_embeddings(batch):\n",
    "    # ensure inputs are tensors; convert if necessary\n",
    "    input_ids = torch.tensor(batch['input_ids'])\n",
    "    attention_mask = torch.tensor(batch['attention_mask'])\n",
    "\n",
    "    # forward pass, no gradient calculation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # return embeddings; convert tensors to numpy arrays\n",
    "    return {'embeddings': outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy()}\n",
    "\n",
    "# run function\n",
    "embeddings = tokenized_dataset.map(extract_embeddings, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save into .pkl file\n",
    "with open('../data/out/embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"opacity: 0.5\">\n",
    "\n",
    "### Emotional intensity score\n",
    "\n",
    "-- *Reduce dimensionality*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert embeddings into a single array\n",
    "all_embeddings = np.vstack([batch['embeddings'] for batch in embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimensionality to 10 dimensions\n",
    "pca = PCA(n_components=10)\n",
    "reduced_embeddings = pca.fit_transform(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scores\n",
    "scores = np.linalg.norm(reduced_embeddings, axis=1)\n",
    "\n",
    "# single score for each press release, indicating its intensity based on the PCA-reduced embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with scores\n",
    "df_scores = pd.DataFrame({\n",
    "    'score': scores\n",
    "}, index=df_sample.index)\n",
    "\n",
    "# merge scores df onto original df\n",
    "df_clean = df_sample.join(df_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as .csv\n",
    "#df_clean.to_csv(\"../data/out/df_clean.csv\", index=False)\n",
    "\n",
    "# load `df_clean` data\n",
    "#df_clean = pd.read_csv(\"../data/out/df_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- *Top / bottom scores*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 scores\n",
    "df_top10 = df_clean.sort_values(by='score', ascending=False).head(10)\n",
    "\n",
    "# bottom 10 scores\n",
    "df_bottom10 = df_clean.sort_values(by='score', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
